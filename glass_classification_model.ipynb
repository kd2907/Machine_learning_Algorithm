{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glass Classification using xgboost\n",
    "\n",
    "###### This is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               RI          Na          Mg          Al          Si           K  \\\n",
      "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
      "mean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \n",
      "std      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \n",
      "min      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n",
      "25%      1.516523   12.907500    2.115000    1.190000   72.280000    0.122500   \n",
      "50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n",
      "75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \n",
      "max      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n",
      "\n",
      "               Ca          Ba          Fe        Type  \n",
      "count  214.000000  214.000000  214.000000  214.000000  \n",
      "mean     8.956963    0.175047    0.057009    2.780374  \n",
      "std      1.423153    0.497219    0.097439    2.103739  \n",
      "min      5.430000    0.000000    0.000000    1.000000  \n",
      "25%      8.240000    0.000000    0.000000    1.000000  \n",
      "50%      8.600000    0.000000    0.000000    2.000000  \n",
      "75%      9.172500    0.000000    0.100000    3.000000  \n",
      "max     16.190000    3.150000    0.510000    7.000000  \n"
     ]
    }
   ],
   "source": [
    "# peview of the data\n",
    "data = pd.read_csv(\"glass.csv\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
      "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
      "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
      "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
      "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
      "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1\n"
     ]
    }
   ],
   "source": [
    "# let's print the head of the data.\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are no null values, so we don't have to worry about that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    76\n",
      "1    70\n",
      "7    29\n",
      "3    17\n",
      "5    13\n",
      "6     9\n",
      "Name: Type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# As we have observed from the type(label) of the data, that there is 7 classes, but from class 4 there is no datapoints.\n",
    "print(data[\"Type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a function to load the dataset from the csv file and split the dataset into train and test set(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:(142, 9)\n",
      "Y_train shape:(142,)\n",
      "X_test shape:(72, 9)\n",
      "Y_test shape:(72,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "def load_dataset(csv_name = \"glass.csv\", train_test_split = 2/3, is_normalization=True ):\n",
    "    dataset = pd.read_csv(csv_name)\n",
    "    dataset_shuffle=dataset.iloc[np.random.permutation(len(dataset))]\n",
    "    dataset=dataset_shuffle.reset_index(drop=True)\n",
    "    X, Y = dataset[[\"RI\", \"Na\", \"Mg\", \"Al\", \"Si\", \"K\", \"Ca\", \"Ba\", \"Fe\"]].values, dataset[[\"Type\"]].values\n",
    "    num_train = int(len(X)*train_test_split)\n",
    "    X_train, Y_train = X[:num_train], Y[:num_train]\n",
    "    X_test, Y_test = X[num_train:], Y[num_train:]\n",
    "    Y_train, Y_test = Y_train.reshape(-1,), Y_test.reshape(-1,)\n",
    "    if is_normalization:\n",
    "        x_mu, x_sigma = np.mean(X_train), np.std(X_train)\n",
    "        y_mu, y_sigma = np.mean(Y_train), np.std(Y_train)\n",
    "        X_train = (X_train - x_mu) / x_sigma \n",
    "        X_test = (X_test - x_mu) / x_sigma\n",
    "        stats = (x_mu, y_mu, x_sigma, y_sigma)\n",
    "    else:\n",
    "        stats = ()\n",
    "        \n",
    "    return X_train, Y_train, X_test, Y_test, stats\n",
    "\n",
    "# split the dataset in train and test\n",
    "train_test_split = 2/3   \n",
    "X_train, Y_train, X_test, Y_test, stats = load_dataset(csv_name = \"glass.csv\", train_test_split = train_test_split, is_normalization=False)\n",
    "\n",
    "print(\"X_train shape:{}\\nY_train shape:{}\\nX_test shape:{}\\nY_test shape:{}\".format(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "clf = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the feature importance, Confusion matrix, Classification report.\n",
    "#### A confusion matrix is a table, which is oftenly used to describe the performance of the classifier on the test data, when true labels are given for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features importance: [0.18766999 0.14188577 0.16999093 0.11332729 0.07298277 0.09972802\n",
      " 0.13236627 0.03762466 0.0444243 ]\n",
      "=== Confusion Matrix ===\n",
      "[[22  2  0  0  0  0]\n",
      " [ 4 23  1  0  0  0]\n",
      " [ 2  1  0  0  0  0]\n",
      " [ 0  0  0  3  0  1]\n",
      " [ 0  0  0  0  3  0]\n",
      " [ 0  0  0  0  0 10]]\n",
      "\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.92      0.85        24\n",
      "           2       0.88      0.82      0.85        28\n",
      "           3       0.00      0.00      0.00         3\n",
      "           5       1.00      0.75      0.86         4\n",
      "           6       1.00      1.00      1.00         3\n",
      "           7       0.91      1.00      0.95        10\n",
      "\n",
      "   micro avg       0.85      0.85      0.85        72\n",
      "   macro avg       0.76      0.75      0.75        72\n",
      "weighted avg       0.83      0.85      0.83        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"features importance:\", clf.feature_importances_)\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(Y_test, Y_pred))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.72222222222221\n"
     ]
    }
   ],
   "source": [
    "# calculate Accuracy\n",
    "accuracy = np.zeros((len(Y_test),1))\n",
    "for i in range(len(Y_pred)):\n",
    "    if Y_pred[i] == Y_test[i]:\n",
    "        accuracy[i] = 1\n",
    "Accuracy = (np.sum(accuracy)/len(accuracy)*100)  \n",
    "print(\"Accuracy:\" ,Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: We can increase the accuracy of the classifier with the help of hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
